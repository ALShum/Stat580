\documentclass{article}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[margin=0.70in]{geometry}
\setlength{\parindent}{0in}

\title{Stat580 - Homework 1}
\author{Alex Shum}
\begin{document}
\maketitle

\section*{Problem 1}
$X_1, X_2, \dots, X_n \sim Unif(0,1)$.  \\
$(\Sigma_{i=1}^n X_i) mod 1 = \Sigma_{i=1}^n X_i - \lfloor \Sigma_{i=1}^n \rfloor \sim Unif(0,1)$. \\
\\
Proof: By induction: clearly for $n=1$ we have that $(\Sigma_{i=1}^n X_i) mod 1 \sim Unif(0,1)$.  Suppose for some $n \ge 1$ we have that $(\Sigma_{i=1}^n X_i) mod 1 \sim Unif(0,1)$.  For $n + 1$ we have that $(\Sigma_{i=1}^{n+1}X_i)mod1 = (\Sigma_{i=1}^n X_i + X_{n+1}) mod 1$.  Both of $\Sigma_{i=1}^n X_i$ and $X_{n+1}$ are uniform(0,1) and the sum of two uniform(0,1) distributions is the triangular distribution: $f(x) = I(0\le x \le 1)x + I(1 \le x \le 2)(2-x)$.  The modulus applies to $I(1 \le x \le 2)$ and it results in a uniform distribution.

\section*{Problem 2}
Let $F$ be a cumulative distribution function and let $F^{-1} = min\{x | F(x) \ge u\}$.  If $U \sim Unif(0,1)$ then $F^{-1}(U) \sim F$.  We start with the cumulative distribution function for $F^{-1}(U)$: $P(F^{-1}(U) \le x)$
\begin{align*}
\text{Applying F to both sides (F is monotonic): }& P(F^{-1}(U) \le x) = P(U \le F(x))\\
\text{But since U is uniform: }& P(U \le F(x)) = F(x)
\end{align*}

\section*{Problem 3}
\subsection*{Part a}
We know that $U_1, U_2 \sim Unif(0,1)$ and $X = \sqrt{-2log(U_1)}cos(2\pi U_2)$ and $Y = \sqrt{-2log(U_1)}sin(2\pi U_1)$.  We will transform $U_1$ and $U_2$ using the above functions and show that it yields a normal.\\
\begin{align*}
X = \sqrt{-2log(U_1)}cos(2\pi U_2) &\text{ and } Y = \sqrt{-2log(U_1)}sin(2\pi U_1) \\
X^2 + Y^2 = -2log(U_1) &\longrightarrow U_1 = exp\{\frac{-1}{2} (X^2 + Y^2) \} \\
\frac{Y}{X} = tan(2\pi U_2) &\longrightarrow U_2 = \frac{1}{2\pi} tan^{-1}(\frac{Y}{X}) \\
|J| = |det \begin{bmatrix}\frac{\partial U_1}{\partial X} & \frac{\partial U_1}{\partial Y} \\ 
                          \frac{\partial U_2}{\partial X} & \frac{\partial U_2}{\partial Y} \end{bmatrix}|
   &= |\begin{bmatrix} exp\{ \frac{-1}{2}(X^2 + Y^2) \} (-X) & exp\{ \frac{-1}{2}(X^2 + Y^2) \} (-Y) \\ 
                       \frac{1}{2\pi} \frac{X^2}{X^2 + Y^2} \frac{-Y}{X^2} & 
                       \frac{1}{2\pi} \frac{X^2}{X^2 + Y^2} \frac{1}{X}\end{bmatrix}| \\
                      &= \frac{1}{2\pi} \frac{X^2}{X^2 + Y^2} exp\{-\frac{X^2 + Y^2}{2} \} (1 + \frac{Y^2}{X^2}) \\
   &= \frac{1}{2\pi} exp\{-\frac{X^2 + Y^2}{2} \} \\
   &= \frac{1}{\sqrt{2\pi}}exp\{-X^2 /2 \} \frac{1}{\sqrt{2\pi}}exp\{-Y^2 / 2 \} 
\end{align*}
Thus we have that $f_{X,Y}(x,y) = \frac{1}{\sqrt{2\pi}}exp\{-x^2 /2 \} \frac{1}{\sqrt{2\pi}}exp\{-y^2 / 2 \}$ thus we have two independent standard normal variables.

\subsection*{Part b}
Since we only accept when $V_1^2 + V_2^2 < 1$ with $V_1^2$ and $V_2^2$ uniform(-1,1) then our acceptance probability is $\pi/4$.  The acceptance region is a radius 1 circle inside a 2 x 2 box.  Since we reject if $V_1^2 + V_2^2 > 1$ then this means that $W = V_1^2 + V_2^2$ is uniform(0,1) and the angle of any point generated in the circle is $uniform(0, 2\pi)$.  Generating a point inside the circle, using trigonometry we can relate the angle from the origin and the coordinates of the points by $sin(\theta) = \frac{V_2}{sqrt{W}}$ and $cos(\theta) = \frac{V_1}{sqrt{W}}$.  Thus from the Box-Muller algorithm we can see that $X = \sqrt{-2log(W)}\frac{V_1}{sqrt{W}}$ and $Y = \sqrt{-2log(W)}\frac{V_2}{sqrt{W}}$ must both be normal(0,1).

\section*{Problem 4}
<<sim, echo=FALSE, cache=TRUE, results='none'>>=
library(ggplot2)
d = 1:20
tail = function(d) {
  U1 = runif(100000,0,1)
  U2 = runif(100000,0,1)
  X = sqrt(d^2 - 2*log(U1))
  sum(U2*X <= d)/100000
}

naive = function(d) {
  R = rnorm(100000,0,1)
  sum(R <= d)/100000
}

a = sapply(d, tail)
b = sapply(d, naive)
@
We sampled from a standard normal tail as follows: we generated 1,000,000 iid samples from $U_1 \sim Unif(0,1)$ and 1,000,000 iid samples from $U_2 \sim Unif(0,1)$.  Then we accept if $U_2 \sqrt{d^2 - 2log(U_1)} \le d$.  We tried $d$ from 1 to 20.  We repeated this experiment by generating 1,000,000 iid samples from $X \sim N(0,1)$ and we accepted if $X \le d$.  \\
\\
We found that when we sample directly from standard normal we accept 100$\%$ of the time when $d > 4$.  When we sampled using the uniforms as described above we found that as $d$ increases the acceptance probability increases but even with $d = 20$ we did not have a 100$\%$ acceptance rate.  With 1,000,000 samples we still rejected about 2200 times.

\section*{Problem 5}
\subsection*{Part a}
The following is a basic outline of our algorithm.  $(A,R,C)$ are the same triples as described in the prompt.  Note that we index the matrix and the vector starting from $index = 0$.  If we want $y_j$, the $j$th element of $y = Wx$.  We must find the $j$th row of our matrix $W$ and multiply with the correct element in vector $x$:
\begin{enumerate}
\item Iterate through the columns (elements in $C$).
\item For each column $i$, find the rows of non-zero elements.
\item If the element in row $j$ is non-zero we multiply it with the $i$th element in $x$.
\end{enumerate}
We include the following pseudo code:
\begin{verbatim}
sum = 0;
r = correct row;
for(int i = 0; i < c.length - 1; i++) {
  for(int j = C[i]; j < C[i + 1]; j++) {
    if(R[j] == r) sum = sum + x[i]*A[j];
  }
}
return(sum)
\end{verbatim}

\subsection*{Part b}
Our strategy is similar to our method in part $a$.  However, with a symmetric matrix we will only store the lower-triangular part of the matrix.  $A$ stores the elements in column-wise order; but only the elements in the diagonal and lower-triangular part of the matrix.  $R$ stores the corresponding row of the elements in $A$.  And elements in $C$ indicate the element in $A$ that starts the columns of the lower triangular part of our matrix.\\
\\
We follow the same outline as above, the key difference is that when we check an element we also check it's reflection: when we check if element (i,j), we will also check element (j,i).  We include the following pseudo code:
\begin{verbatim}
sum = 0;
r = correct row;
for(int i = 0; i < c.length - 1; i++) {
  col = i;
  for(int j = c[i]; j < c[i + 1]; j++) {
    row = R[j];
    if(row == r) sum = sum + y[i]*A[j];
    if(col != row && col == r) sum = sum + y[row]*A[j];
  }
}
\end{verbatim}


\section*{Problem 6}
\subsection*{Part a}
Since there are no changes to population 3 and 4, the only changes to the WSS are in the $\hat{\mu}_1$ and $\hat{\mu}_2$ terms.  We calculate the change in WSS below when we move $W_{n_W}$ from population 1 to population 2.  First we calculate the difference in WSS for population 1:
\begin{align*}
\text{Denote: } \hat{\mu_1} = \frac{n_U \bar{U} + n_W \bar{W}}{n_U + n_W} \text{ and } \hat{\mu_2} = \frac{n_V \bar{V} + n_X \bar{X}}{n_V + n_X}\\
\text{Denote: } \hat{\mu_1}^* = \frac{n_U \bar{U} + n_W \bar{W} - W_{n_W}}{n_U + n_W - 1}  \text{ and } \hat{\mu_2}^* = \frac{n_V \bar{V} + n_X \bar{X} + W_{n_W}}{n_V + n_X + 1}\\
\Sigma_{i=1}^{n_U}||U_i - \hat{\mu_1}^*||^2 + \Sigma_{i=1}^{n_W}||W_i - \hat{\mu_1}^*||^2 - ||W_{n_W} - \hat{\mu_1}^*||^2 - \Sigma_{i=1}^{n_U}||U_i - \hat{\mu_1}||^2 - \Sigma_{i=1}^{n_W}||W_i - \hat{\mu}||^2\\
=\Sigma_{i=1}^{n_U}||U_i - \hat{\mu_1}||^2 + n_U||\hat{\mu_1} - \hat{\mu_1}^*||^2 + \Sigma_{i=1}^{n_W}||W_i - \hat{\mu_1}||^2 + n_W||\hat{\mu_1} - \hat{\mu_1}^*||^2 - \\
||W_{n_W} - \hat{\mu_1}^*||^2 - \Sigma_{i=1}^{n_U}||U_i - \hat{\mu_1}||^2 - \Sigma_{i=1}^{n_W}||W_i - \hat{\mu}||^2\\
=n_U||\hat{\mu_1} - \hat{\mu_1}^*||^2 + n_W||\hat{\mu_1} - \hat{\mu_1}^*||^2 - ||W_{n_W} - \hat{\mu_1}^*||^2\\
= n_U||\frac{n_U(W_{n_W} - \bar{U}) + n_W(W_{n_W} - \bar{W})}{(n_U + n_W)(n_U + n_W - 1)}||^2 + n_W||\frac{n_U(W_{n_W} - \bar{U}) + n_W(W_{n_W} - \bar{W})}{(n_U + n_W)(n_U + n_W - 1)}||^2 - \\
||\frac{n_U(W_{n_W} - \bar{U}) + n_W(W_{n_W} - \bar{W})}{(n_U + n_W - 1)}||^2 \\
= (\frac{1}{n_U + n_W} - 1)||\frac{n_U(W_{n_W} - \bar{U}) + n_W(W_{n_W} - \bar{W})}{n_U + n_W - 1}||^2
\end{align*}
The difference in WSS for population 2:
\begin{align*}
\text{Denote: } \hat{\mu_2} = \frac{n_V \bar{V} + n_X \bar{X}}{n_V + n_X} \text{ and } \hat{\mu_2}^* = \frac{n_V \bar{V} + n_X \bar{X} + W_{n_W}}{n_V + n_X + 1} \\
\Sigma_{i=1}^{n_V}||V_i - \hat{\mu_2}^*||^2 + \Sigma_{i=1}^{n_X}||X_i - \hat{\mu_2}^*||^2 + ||W_{n_W} - \hat{\mu_2}^*||^2 - \Sigma_{i=1}^{n_V}||V_i - \hat{\mu_2}||^2 - \Sigma_{i=1}^{n_X}||X_i - \hat{\mu_2}||^2 \\
= \Sigma_{i=1}^{n_V}||V_i - \hat{\mu_2}||^2 + n_V||\hat{\mu_2} - \hat{\mu_2}^*||^2 + \Sigma_{i=1}^{n_X}||X_i - \hat{\mu_2}||^2 +  n_X||\hat{\mu_2} - \hat{\mu}^*||^2 +  \\
||W_{n_W} - \hat{\mu_2}^*||^2 - \Sigma_{i=1}^{n_V}||V_i - \hat{\mu_2}||^2 - \Sigma_{i=1}^{n_X}||X_i - \hat{\mu_2}||^2\\
=  n_V||\hat{\mu_2} - \hat{\mu_2}^*||^2 + n_X||\hat{\mu_2} - \hat{\mu}^*||^2 + ||W_{n_W} - \hat{\mu_2}^*||^2\\
= \frac{1}{n_V + n_X}||\frac{n_V(\bar{V} - W_{n_W}) + n_X(\bar{X} - W_{n_W})}{n_V + n_X + 1}||^2 + ||\frac{n_V(\bar{V} - W_{n_W}) + n_X(\bar{X} - W_{n_W})}{n_V + n_X + 1}||^2\\
= (\frac{1}{n_V + n_X} + 1)||\frac{n_V(\bar{V} - W_{n_W}) + n_X(\bar{X} - W_{n_W})}{n_V + n_X + 1}||^2
\end{align*}

\subsection*{Part b}
Since there are no changes to population 2 and 4, the only changes to the WSS are in the $\hat{\mu}_1$ and $\hat{\mu}_3$ terms.  We calculate the change in WSS below when we move $W_{n_W}$ from population 1 to population 3.  First we calculate the difference in WSS for population 1:
\begin{align*}
\text{Denote: } \hat{\mu_1} = \frac{n_U\bar{U} + n_W\bar{W}}{n_U + n_W} \text{ and } \hat{\mu_1}^* = \frac{n_U\bar{U} + n_W\bar{W} - W_{n_W}}{n_U + n_W - 1}\\
\Sigma_{i=1}^{n_U}||U_i - \hat{\mu_1}^*||^2 + \Sigma_{i=1}^{n_W}||W_i - \hat{\mu_1}^*||^2 - ||W_{n_W} - \hat{\mu_1}^*||^2 - \Sigma_{i=1}^{n_U}||U_i - \hat{\mu_1}||^2 - \Sigma_{i=1}^{n_W}||W_i - \hat{\mu_1}||^2\\
= \Sigma_{i=1}^{n_U}||U_i - \hat{\mu_1}||^2 + n_U||\hat{\mu_1} - \hat{\mu_1}^*||^2 + \Sigma_{i=1}^{n_W}||W_i - \hat{\mu_1}||^2 + n_W||\hat{\mu_1} - \hat{\mu_1}^*||^2 - ||W_{n_W} - \hat{\mu_1}^*||^2 - \\
\Sigma_{i=1}^{n_U}||U_i - \hat{\mu_1}||^2 - \Sigma_{i=1}^{n_W}||W_i - \hat{\mu_1}||^2\\
= (n_U + n_W)||\hat{\mu_1} - \hat{\mu_1}^*||^2 - ||W_{n_W} - \hat{\mu_1}^*||^2 \\
= \frac{1}{n_U + n_W}||\frac{n_W(W_{n_W} - \bar{W}) + n_U(W_{n_W} - \bar{U})}{n_U + n_W - 1}||^2 - ||\frac{n_U(W_{n_W} - \bar{U}) + n_W(W_{n_W} - \bar{W})}{n_U + n_W - 1}||^2\\
= (\frac{1}{n_U + n_W} - 1)||\frac{n_U(W_{n_W} - \bar{U}) + n_W(W_{n_W} - \bar{W})}{n_U + n_W - 1}||^2
\end{align*}
The difference in WSS for population 3:
\begin{align*}
\text{Denote: } \bar{Y} = \frac{\Sigma_{i=1}^{n_Y}}{n_Y} \text{ and } \bar{Y}^* = \frac{n_Y\bar{Y} + W_{n_W}}{n_Y + 1}\\
\Sigma_{i=1}^{n_Y}||Y_i = \bar{Y}^*||^2 + ||W_{n_W} - \bar{Y}^*||^2 - \Sigma_{i=1}^{n_Y}||Y_i - \bar{Y}||^2\\
\Sigma_{i=1}^{n_Y}||Y_i = \bar{Y}||^2 + n_Y||\bar{Y} - \bar{Y}^*||^2 + ||W_{n_W} - \bar{Y}^*||^2 - \Sigma_{i=1}^{n_Y}||Y_i - \bar{Y}||^2\\
\frac{n_Y}{(n_Y + 1)^2}||\bar{Y} - W_{n_W}||^2 + (\frac{n_Y}{n_Y + 1})^2||W_{n_W} - \bar{Y}||^2\\
\frac{n_Y}{n_Y + 1}||\bar{Y} - W_{n_W}||^2
\end{align*}

\subsection*{Part c}
Since there are no changes to population 1 and 2, the only changes to the WSS is in $\Sigma_{i=1}^{n_Y}||Y_i - \hat{\mu}_3||^2$ and $\Sigma_{i=1}^{n_Z}||Z_i - \hat{\mu}_4||^2$.  We calculate the change in WSS below when we move $Y_{n_Y}$ from population 3 to population 4.  First we calculate the difference in WSS for population 4:
\begin{align*}
\text{Denote: } \bar{Z} = \frac{\Sigma_{i=1}^{n_Z}Z_i}{n_Z} \text{ and } \bar{Z}^* = \frac{n\bar{Z} + Y_{nY}}{n_Z + 1} \\ 
\Sigma_{i=1}^{n_Z}||Z_i - \bar{Z}^*||^2 + ||Y_{n_Y} - \bar{Z}^*||^2 - \Sigma_{i=1}^{n_Z}||Z_i - \bar{Z}||^2\\
= \Sigma_{i=1}^{n_Z}||Z_i - \bar{Z}||^2 + n_{Z}||\bar{Z} - \bar{Z}^*||^2 + ||Y_{n_Y} - \bar{Z}^*||^2 - \Sigma_{i=1}^{n_Z}||Z_i - \bar{Z}||^2\\
= n_{Z}||\bar{Z} - \bar{Z}^*||^2 + ||Y_{n_Y} - \bar{Z}^*||^2\\
= \frac{n_Z}{(n_Z + 1)^2}||\bar{Z} - Y_{n_Y}||^2 + (\frac{n_Z}{n_Z + 1})^2 ||Y_{n_Y} - \bar{Z}||^2\\
= \frac{n_Z}{n_Z + 1}||\bar{Z} - Y_{n_Y}||^2
\end{align*}
The difference in WSS for population 3:
\begin{align*}
\text{Denote: } \bar{Y} = \frac{\Sigma_{i=1}^{n_Y}Y_i}{n_Y} \text{ and }\bar{Y}^* = \frac{n_Y \bar{Y} - Y_{n_Y}}{n_Y - 1}\\
\Sigma_{i=1}^{n_Y}||Y_i - \bar{Y}^*||^2 - ||Y_{n_Y} - \bar{Y}^*||^2 - \Sigma_{i=1}^{n_Y}||Y_i - \bar{Y}||^2 \\
= \Sigma_{i=1}^{n_Y}||Y_i - \bar{Y}||^2 + n_Y||\bar{Y} - \bar{Y}^*||^2 - ||Y_{n_Y} - \bar{Y}^*||^2 - \Sigma_{i=1}^{n_Y}||Y_i - \bar{Y}||^2\\
= n_Y||\bar{Y} - \bar{Y}^*||^2 - ||Y_{n_Y} - \bar{Y}^*||^2 \\
= \frac{n_Y}{(n_Y - 1)^2}||Y_{n_Y} - \bar{Y}||^2 - (\frac{n_Y}{n_Y - 1})^2 ||Y_{n_Y} - \bar{Y}||^2 \\
= \frac{n_Y}{n_Y - 1}||Y_{n_Y} - \bar{Y}||^2
\end{align*}

\section*{Problem 7}
See readme.txt file in /class/stat580/ashum/homework1/
\subsection*{Part a}
Code available on impact2.stat.iastate.edu under /class/stat580/ashum/homework1/temp.c
\subsection*{Part b}
Code available on impact2.stat.iastate.edu under /class/stat580/ashum/homework1/mult.c

\end{document}